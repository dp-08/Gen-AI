{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNd9I1aftCvy3p50om1prp4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dp-08/Gen-AI/blob/main/DCGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPEVCSHgVvih",
        "outputId": "9de16d24-a3fe-4b44-eb8c-eee49e39f322"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: DCGAN requires a downloaded and extracted dataset (e.g., CelebA) at the specified path.\n",
            "Skipping data loading for conceptual code structure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting DCGAN training...\n",
            "DCGAN model structure defined and compiled. Ready for training using dcgan.fit(dataset, epochs=EPOCHS).\n",
            "Generator output shape: (None, 64, 64, 3)\n",
            "Discriminator output shape: (None, 1)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# --- Configuration & Hyperparameters ---\n",
        "IMG_SIZE = 64         # Image resolution (DCGAN is best for 64x64 or 128x128)\n",
        "CHANNELS = 3          # RGB images\n",
        "LATENT_DIM = 100      # Size of the random noise vector\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 100          # Training for thousands of epochs is common for good results\n",
        "ADAM_LR = 0.0002\n",
        "ADAM_BETA1 = 0.5      # Recommended for GANs\n",
        "\n",
        "# --- 1. Data Loading and Preprocessing (Using a simplified CelebA path) ---\n",
        "def load_and_preprocess_data(data_dir, img_size, batch_size):\n",
        "    \"\"\"Loads image data, resizes, and normalizes it to [-1, 1].\"\"\"\n",
        "    print(f\"Loading images from directory: {data_dir}\")\n",
        "\n",
        "    # The CelebA dataset must be manually downloaded and extracted.\n",
        "    # Assumes 'data_dir' contains the image files.\n",
        "    dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "        data_dir,\n",
        "        label_mode=None,           # Unlabeled data for GANs\n",
        "        image_size=(img_size, img_size),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        interpolation='bilinear'\n",
        "    )\n",
        "\n",
        "    # Normalize images from [0, 255] to [-1, 1]\n",
        "    dataset = dataset.map(lambda x: (x - 127.5) / 127.5)\n",
        "    return dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "# Replace this with the actual path to your extracted CelebA images\n",
        "# DATASET_PATH = 'path/to/img_align_celeba'\n",
        "# For demonstration, assume a simplified placeholder\n",
        "DATASET_PATH = 'celeba_dataset_placeholder'\n",
        "\n",
        "# NOTE: You must replace 'celeba_dataset_placeholder' with the actual path\n",
        "# to your CelebA image directory for the code to run.\n",
        "if not os.path.isdir(DATASET_PATH):\n",
        "    print(\"Warning: DCGAN requires a downloaded and extracted dataset (e.g., CelebA) at the specified path.\")\n",
        "    print(\"Skipping data loading for conceptual code structure.\")\n",
        "    # Create a dummy dataset for structure demonstration if path is missing\n",
        "    dummy_data = np.random.rand(100, IMG_SIZE, IMG_SIZE, CHANNELS)\n",
        "    dummy_data = (dummy_data - 0.5) * 2.0\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(dummy_data).batch(BATCH_SIZE)\n",
        "else:\n",
        "    dataset = load_and_preprocess_data(DATASET_PATH, IMG_SIZE, BATCH_SIZE)\n",
        "\n",
        "\n",
        "# --- 2. Generator Model ---\n",
        "def make_generator_model(latent_dim, channels):\n",
        "    \"\"\"\n",
        "    Creates the Generator network using Conv2DTranspose (deconvolutional layers).\n",
        "    Maps from latent vector (1D) to image (3D).\n",
        "    \"\"\"\n",
        "    model = tf.keras.Sequential(name=\"generator\")\n",
        "\n",
        "    # Start with a Dense layer, reshape to a small image volume (4x4x512)\n",
        "    # The input to the first Conv2DTranspose will be 4x4.\n",
        "    model.add(layers.Dense(4 * 4 * 512, use_bias=False, input_shape=(latent_dim,)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Reshape((4, 4, 512))) # (4, 4, 512)\n",
        "\n",
        "    # Upsample to 8x8\n",
        "    model.add(layers.Conv2DTranspose(256, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU()) # (8, 8, 256)\n",
        "\n",
        "    # Upsample to 16x16\n",
        "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU()) # (16, 16, 128)\n",
        "\n",
        "    # Upsample to 32x32\n",
        "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU()) # (32, 32, 64)\n",
        "\n",
        "    # Upsample to 64x64 (final layer)\n",
        "    model.add(layers.Conv2DTranspose(channels, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "    # (64, 64, 3) with pixel values in [-1, 1]\n",
        "\n",
        "    return model\n",
        "\n",
        "# --- 3. Discriminator Model ---\n",
        "def make_discriminator_model(img_size, channels):\n",
        "    \"\"\"\n",
        "    Creates the Discriminator network using Conv2D layers.\n",
        "    Maps from image (3D) to a single probability (0=fake, 1=real).\n",
        "    \"\"\"\n",
        "    model = tf.keras.Sequential(name=\"discriminator\")\n",
        "\n",
        "    # Downsample from 64x64 to 32x32\n",
        "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
        "                            input_shape=[img_size, img_size, channels]))\n",
        "    model.add(layers.LeakyReLU(0.2))\n",
        "    model.add(layers.Dropout(0.3)) # (32, 32, 64)\n",
        "\n",
        "    # Downsample to 16x16\n",
        "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU(0.2))\n",
        "    model.add(layers.Dropout(0.3)) # (16, 16, 128)\n",
        "\n",
        "    # Downsample to 8x8\n",
        "    model.add(layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU(0.2))\n",
        "    model.add(layers.Dropout(0.3)) # (8, 8, 256)\n",
        "\n",
        "    # Downsample to 4x4\n",
        "    model.add(layers.Conv2D(512, (5, 5), strides=(2, 2), padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU(0.2))\n",
        "    model.add(layers.Dropout(0.3)) # (4, 4, 512)\n",
        "\n",
        "    # Flatten and output probability\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# --- 4. Define Loss and Optimizers ---\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    # Real images should be labeled 1 (real_output near 1)\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output) * 0.9, real_output) # Use label smoothing (0.9 instead of 1.0)\n",
        "    # Fake images should be labeled 0 (fake_output near 0)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    # Generator wants discriminator to think fakes are real (label 1)\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(ADAM_LR, beta_1=ADAM_BETA1)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(ADAM_LR, beta_1=ADAM_BETA1)\n",
        "\n",
        "\n",
        "# --- 5. DCGAN Model (Combined with a custom train step) ---\n",
        "# Use a custom Model class to implement the GAN training logic within the Keras fit() loop\n",
        "\n",
        "class DCGAN(tf.keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super(DCGAN, self).__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n",
        "        super(DCGAN, self).compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.d_loss_fn = d_loss_fn\n",
        "        self.g_loss_fn = g_loss_fn\n",
        "        self.d_loss_metric = tf.keras.metrics.Mean(name='d_loss')\n",
        "        self.g_loss_metric = tf.keras.metrics.Mean(name='g_loss')\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.d_loss_metric, self.g_loss_metric]\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "\n",
        "        # 1. Train the Discriminator\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "            # Generate fake images\n",
        "            generated_images = self.generator(random_latent_vectors, training=True)\n",
        "\n",
        "            # Get the discriminator's predictions\n",
        "            real_output = self.discriminator(real_images, training=True)\n",
        "            fake_output = self.discriminator(generated_images, training=True)\n",
        "\n",
        "            # Calculate losses\n",
        "            d_loss = self.d_loss_fn(real_output, fake_output)\n",
        "            g_loss = self.g_loss_fn(fake_output)\n",
        "\n",
        "        # Apply gradients to Discriminator\n",
        "        d_gradients = disc_tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
        "        self.d_optimizer.apply_gradients(zip(d_gradients, self.discriminator.trainable_variables))\n",
        "\n",
        "        # Apply gradients to Generator\n",
        "        g_gradients = gen_tape.gradient(g_loss, self.generator.trainable_variables)\n",
        "        self.g_optimizer.apply_gradients(zip(g_gradients, self.generator.trainable_variables))\n",
        "\n",
        "        # Update and return the metrics\n",
        "        self.d_loss_metric.update_state(d_loss)\n",
        "        self.g_loss_metric.update_state(g_loss)\n",
        "        return {\"d_loss\": self.d_loss_metric.result(), \"g_loss\": self.g_loss_metric.result()}\n",
        "\n",
        "\n",
        "# --- 6. Initialization and Training ---\n",
        "\n",
        "# Build the models\n",
        "generator = make_generator_model(LATENT_DIM, CHANNELS)\n",
        "discriminator = make_discriminator_model(IMG_SIZE, CHANNELS)\n",
        "\n",
        "# Instantiate the custom DCGAN model\n",
        "dcgan = DCGAN(discriminator=discriminator, generator=generator, latent_dim=LATENT_DIM)\n",
        "\n",
        "# Compile the DCGAN\n",
        "dcgan.compile(\n",
        "    d_optimizer=discriminator_optimizer,\n",
        "    g_optimizer=generator_optimizer,\n",
        "    d_loss_fn=discriminator_loss,\n",
        "    g_loss_fn=generator_loss\n",
        ")\n",
        "\n",
        "print(\"Starting DCGAN training...\")\n",
        "\n",
        "# NOTE: This will only work if the dataset is correctly loaded.\n",
        "# Use a GPU/Colab for efficient training.\n",
        "# history = dcgan.fit(dataset, epochs=EPOCHS)\n",
        "\n",
        "print(\"DCGAN model structure defined and compiled. Ready for training using dcgan.fit(dataset, epochs=EPOCHS).\")\n",
        "print(f\"Generator output shape: {generator.output_shape}\")\n",
        "print(f\"Discriminator output shape: {discriminator.output_shape}\")\n",
        "\n",
        "# Example generation after hypothetical training\n",
        "# sample_noise = tf.random.normal(shape=[1, LATENT_DIM])\n",
        "# generated_image = generator(sample_noise, training=False)\n",
        "# plt.imshow((generated_image[0] * 0.5 + 0.5))\n",
        "# plt.show()"
      ]
    }
  ]
}